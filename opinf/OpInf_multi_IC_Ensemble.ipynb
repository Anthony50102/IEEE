{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f729ac-602d-4954-945f-951ff8ba23aa",
   "metadata": {},
   "source": [
    "# End-to-End Opinf for multiple initial conditions\n",
    "## Install notes with current startup script\n",
    "- `pip install -e . --no-deps`  # Install without dependencies first\n",
    "- `pip install -e .`            # Then install with pinned dependencies\n",
    "## What this notebook can do\n",
    "- Compute a POD basis for 1+ intial conditions\n",
    "- Find the operator(s) for $\\gamma_n$, $\\gamma_c$, and state\n",
    "- Use learned operator(s) and POD to compute predictions across unseen data\n",
    "\n",
    "## Configuration Notes\n",
    "\n",
    "### Memory-mapped files\n",
    "Files created in `output_path` during execution:\n",
    "- `memmap_Q_train.dat` - Full training snapshots (step_1 only)\n",
    "- `memmap_Q_test.dat` - Full test snapshots (step_1 only)  \n",
    "\n",
    "### Pipeline control\n",
    "| Setting | Description |\n",
    "|---------|-------------|\n",
    "| `step_1=True, step_2=True` | Full pipeline: compute POD, train ROM, make predictions |\n",
    "| `step_1=False, step_2=True` | Load existing POD, train new ROM |\n",
    "| `step_1=False, step_2=False` | Load existing POD and ROM, make predictions only |\n",
    "\n",
    "### Data truncation\n",
    "Limit snapshots per trajectory for quick tests or memory constraints:\n",
    "```python\n",
    "truncate_data = True\n",
    "truncate_snapshots = 1000      # Option 1: Keep N snapshots\n",
    "# OR\n",
    "truncate_time = 25.0           # Option 2: Keep T time units (uses dt from file)\n",
    "```\n",
    "\n",
    "### Ensemble model selection\n",
    "Two methods for selecting models to include in the ensemble:\n",
    "\n",
    "**Method 1: Top-K Selection**\n",
    "```python\n",
    "model_selection_method = \"top_k\"\n",
    "num_top_models = 20            # Keep the 20 best models\n",
    "```\n",
    "\n",
    "**Method 2: Threshold Selection**\n",
    "```python\n",
    "model_selection_method = \"threshold\"\n",
    "threshold_mean_error = 0.05    # Accept if mean relative error < 5%\n",
    "threshold_std_error = 0.30     # Accept if std relative error < 30%\n",
    "```\n",
    "\n",
    "### Cleanup\n",
    "To free disk space after completion:\n",
    "```python\n",
    "for name in [\"Q_train\", \"Q_test\"]:\n",
    "    cleanup_memmap(name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203cf9ad-b8f5-4323-8495-77a184dd8bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mUsing cluster settings\u001b[0m\n",
      "\u001b[1mNumber of training trajectories: 1\u001b[0m\n",
      "\u001b[1mNumber of test trajectories: 1\u001b[0m\n",
      "\u001b[1mData truncation: DISABLED\u001b[0m\n",
      "\u001b[1mModel selection method: threshold\u001b[0m\n",
      "  -> Selecting models with mean error < 5.0%\n",
      "  -> Selecting models with std error < 30.0%\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from opinf_for_hw.data_proc import *\n",
    "from opinf_for_hw.postproc import *\n",
    "from opinf_for_hw.utils.helpers import loader\n",
    "from opinf_for_hw.utils.opinf_utils import (\n",
    "    bprint,\n",
    "    get_memmap_path,\n",
    "    cleanup_memmap,\n",
    "    get_dt_from_file,\n",
    "    compute_truncation_snapshots,\n",
    "    solve_opinf_difference_model,\n",
    "    TopKModels,\n",
    "    ThresholdModels,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import h5py\n",
    "from IPython import display\n",
    "import xarray as xr\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# GENERAL CONFIGURATION\n",
    "# =============================================================================\n",
    "cluster = True\n",
    "show_animations = False\n",
    "show_plots = True\n",
    "ENGINE = \"h5netcdf\"\n",
    "plt.rcParams['animation.embed_limit'] = 250\n",
    "plt.rcParams[\"image.cmap\"] = \"bwr\"  # Other options: seismic\n",
    "\n",
    "# =============================================================================\n",
    "# PIPELINE CONTROL\n",
    "# =============================================================================\n",
    "step_1 = False  # Compute POD basis from training data\n",
    "step_2 = False  # Train ROM and perform regularization sweep\n",
    "\n",
    "# =============================================================================\n",
    "# MEMORY CONFIGURATION\n",
    "# =============================================================================\n",
    "use_chunked_projection = False  # Set True to project in chunks (lower peak memory)\n",
    "\n",
    "# =============================================================================\n",
    "# DATA TRUNCATION CONFIGURATION\n",
    "# =============================================================================\n",
    "# Set truncate_data = True to limit the number of snapshots used.\n",
    "# Specify EITHER truncate_snapshots OR truncate_time (not both).\n",
    "# If both are set, truncate_snapshots takes priority.\n",
    "# =============================================================================\n",
    "truncate_data = False           # Enable/disable truncation\n",
    "truncate_snapshots = None       # Number of snapshots to keep (e.g., 1000)\n",
    "truncate_time = 200            # Simulation time to keep (e.g., 25.0 time units)\n",
    "default_dt = 0.025              # Default dt if not found in file attributes\n",
    "\n",
    "# =============================================================================\n",
    "# ENSEMBLE MODEL SELECTION CONFIGURATION\n",
    "# =============================================================================\n",
    "# Two methods for selecting models to include in the ensemble:\n",
    "#\n",
    "# Method 1: TOP-K SELECTION (model_selection_method = \"top_k\")\n",
    "#   - Select the k best models ranked by total error\n",
    "#   - Set num_top_models to desired ensemble size\n",
    "#\n",
    "# Method 2: THRESHOLD SELECTION (model_selection_method = \"threshold\")\n",
    "#   - Select all models meeting error criteria\n",
    "#   - Models must satisfy: relative error < threshold for mean AND std\n",
    "#   - Thresholds defined separately for Gamma_n and Gamma_c\n",
    "# =============================================================================\n",
    "model_selection_method = \"threshold\"  # Options: \"top_k\" or \"threshold\"\n",
    "\n",
    "# Top-K parameters\n",
    "num_top_models = 20               # Number of best models to keep\n",
    "\n",
    "# Threshold parameters (used when model_selection_method = \"threshold\")\n",
    "# Relative error thresholds: |pred - ref| / |ref| < threshold\n",
    "threshold_mean_error = 0.05       # Maximum relative error in mean (5%)\n",
    "threshold_std_error = 0.30        # Maximum relative error in std (30%)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD CONFIGURATION\n",
    "# =============================================================================\n",
    "if cluster:\n",
    "    bprint(\"Using cluster settings\")\n",
    "    from config.cluster import *\n",
    "else:\n",
    "    bprint(\"Using local settings\")\n",
    "    from config.local import *\n",
    "\n",
    "if r > svd_save:\n",
    "    bprint(\"Warning! r value larger than svd_save\")\n",
    "\n",
    "# =============================================================================\n",
    "# PRINT CONFIGURATION SUMMARY\n",
    "# =============================================================================\n",
    "bprint(f\"Number of training trajectories: {len(training_files)}\")\n",
    "bprint(f\"Number of test trajectories: {len(test_files)}\")\n",
    "\n",
    "if truncate_data:\n",
    "    if truncate_snapshots is not None:\n",
    "        bprint(f\"Data truncation: {truncate_snapshots} snapshots per trajectory\")\n",
    "    elif truncate_time is not None:\n",
    "        bprint(f\"Data truncation: {truncate_time} time units (dt={default_dt})\")\n",
    "else:\n",
    "    bprint(\"Data truncation: DISABLED\")\n",
    "\n",
    "bprint(f\"Model selection method: {model_selection_method}\")\n",
    "if model_selection_method == \"top_k\":\n",
    "    print(f\"  -> Selecting top {num_top_models} models by total error\")\n",
    "elif model_selection_method == \"threshold\":\n",
    "    print(f\"  -> Selecting models with mean error < {threshold_mean_error:.1%}\")\n",
    "    print(f\"  -> Selecting models with std error < {threshold_std_error:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10356143-021d-448c-a79f-195121a294f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/lib/intel64']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl', '/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/include', '/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/lib']\n",
      "blas_opt_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/lib/intel64']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl', '/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/include', '/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/lib']\n",
      "lapack_mkl_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/lib/intel64']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl', '/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/include', '/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/lib']\n",
      "lapack_opt_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/lib/intel64']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl', '/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/include', '/opt/intel/compilers_and_libraries_2019.5.281/linux/mkl/lib']\n"
     ]
    }
   ],
   "source": [
    "np.__config__.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5052605-5e8c-45ab-832f-1c8b64b6d87d",
   "metadata": {},
   "source": [
    "## Step 1: Compute POD basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ae59a-b4bd-45cc-ab63-b49ec1fc5ff6",
   "metadata": {},
   "source": [
    "### Step 1.1: Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cdf257b-26a4-4436-9c89-2875e6ec1c96",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSkipping data loading (step_1=False, will load POD directly)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_snapshots(file_path, i, dataset_name=\"\", max_snapshots=None):\n",
    "    \"\"\"Load and process a single snapshot file into flattened array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the h5 file.\n",
    "    i : int\n",
    "        Index of this file (for printing).\n",
    "    dataset_name : str\n",
    "        Prefix for print statements.\n",
    "    max_snapshots : int, optional\n",
    "        If set, truncate to this many snapshots.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Q_ic : np.ndarray\n",
    "        Processed array of shape (n_spatial, n_time).\n",
    "    \"\"\"\n",
    "    print(f\"  Loading IC {i+1}: {file_path}\")\n",
    "    fh = xr.open_dataset(file_path, engine=ENGINE, phony_dims=\"sort\")\n",
    "    \n",
    "    # Get density and phi as numpy arrays\n",
    "    density = fh[\"density\"].values\n",
    "    phi = fh[\"phi\"].values\n",
    "    fh.close()\n",
    "    \n",
    "    # Apply truncation if specified\n",
    "    original_n_time = density.shape[0]\n",
    "    if max_snapshots is not None and max_snapshots < original_n_time:\n",
    "        density = density[:max_snapshots]\n",
    "        phi = phi[:max_snapshots]\n",
    "        print(f\"    Truncated: {original_n_time} -> {max_snapshots} snapshots\")\n",
    "    \n",
    "    if i == 0 and show_plots:\n",
    "        plt.imshow(density[0])\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        bprint(f\"{dataset_name}IC{i} shape: {density.shape}\")\n",
    "    \n",
    "    # Handle 2D vs 3D input\n",
    "    if density.ndim == 2:\n",
    "        n_time = density.shape[0]\n",
    "        grid_size = int(np.sqrt(density.shape[1]))\n",
    "        density = density.reshape(n_time, grid_size, grid_size)\n",
    "        phi = phi.reshape(n_time, grid_size, grid_size)\n",
    "    \n",
    "    # Stack and reshape: (time, y, x) -> (n_spatial, time)\n",
    "    Q_ic = np.stack([density, phi], axis=0)  # (2, time, y, x)\n",
    "    del density, phi\n",
    "    \n",
    "    Q_ic = Q_ic.transpose(0, 2, 3, 1)  # (2, y, x, time)\n",
    "    n_field, n_y, n_x, n_time = Q_ic.shape\n",
    "    Q_ic = Q_ic.reshape(n_field * n_y * n_x, n_time)\n",
    "    \n",
    "    print(f\"    Shape: {Q_ic.shape}\")\n",
    "    return Q_ic\n",
    "\n",
    "\n",
    "if step_1:\n",
    "    bprint(\"Reading snapshot(s) - Building memory-mapped arrays\")\n",
    "    \n",
    "    # First pass: determine total sizes (with truncation)\n",
    "    train_timesteps = []\n",
    "    test_timesteps = []\n",
    "    train_truncations = []\n",
    "    test_truncations = []\n",
    "    n_spatial = None\n",
    "    \n",
    "    for file_path in training_files:\n",
    "        with xr.open_dataset(file_path, engine=ENGINE, phony_dims=\"sort\") as fh:\n",
    "            n_time_original = fh[\"density\"].shape[0]\n",
    "            if n_spatial is None:\n",
    "                if fh[\"density\"].ndim == 3:\n",
    "                    n_spatial = 2 * fh[\"density\"].shape[1] * fh[\"density\"].shape[2]\n",
    "                else:\n",
    "                    n_spatial = 2 * fh[\"density\"].shape[1]\n",
    "        \n",
    "        if truncate_data:\n",
    "            max_snaps = compute_truncation_snapshots(\n",
    "                file_path, truncate_snapshots, truncate_time, default_dt\n",
    "            )\n",
    "            n_time = min(n_time_original, max_snaps) if max_snaps else n_time_original\n",
    "        else:\n",
    "            n_time = n_time_original\n",
    "            max_snaps = None\n",
    "        \n",
    "        train_timesteps.append(n_time)\n",
    "        train_truncations.append(max_snaps)\n",
    "    \n",
    "    for file_path in test_files:\n",
    "        with xr.open_dataset(file_path, engine=ENGINE, phony_dims=\"sort\") as fh:\n",
    "            n_time_original = fh[\"density\"].shape[0]\n",
    "        \n",
    "        if truncate_data:\n",
    "            max_snaps = compute_truncation_snapshots(\n",
    "                file_path, truncate_snapshots, truncate_time, default_dt\n",
    "            )\n",
    "            n_time = min(n_time_original, max_snaps) if max_snaps else n_time_original\n",
    "        else:\n",
    "            n_time = n_time_original\n",
    "            max_snaps = None\n",
    "        \n",
    "        test_timesteps.append(n_time)\n",
    "        test_truncations.append(max_snaps)\n",
    "    \n",
    "    total_train_time = sum(train_timesteps)\n",
    "    total_test_time = sum(test_timesteps)\n",
    "    \n",
    "    bprint(f\"Creating memory-mapped arrays: {n_spatial} spatial × {total_train_time} train\")\n",
    "    \n",
    "    # Create memory-mapped arrays\n",
    "    cleanup_memmap(output_path, \"Q_train\")\n",
    "    cleanup_memmap(output_path, \"Q_test\")\n",
    "    \n",
    "    Q_train = np.memmap(\n",
    "        get_memmap_path(output_path, \"Q_train\"), \n",
    "        dtype='float64', mode='w+', shape=(n_spatial, total_train_time)\n",
    "    )\n",
    "    Q_test = np.memmap(\n",
    "        get_memmap_path(output_path, \"Q_test\"), \n",
    "        dtype='float64', mode='w+', shape=(n_spatial, total_test_time)\n",
    "    )\n",
    "    \n",
    "    # Store timestep boundaries\n",
    "    train_boundaries = [0] + list(np.cumsum(train_timesteps))\n",
    "    test_boundaries = [0] + list(np.cumsum(test_timesteps))\n",
    "    \n",
    "    # Load training data\n",
    "    for i, file_path in enumerate(training_files):\n",
    "        Q_ic = load_and_process_snapshots(\n",
    "            file_path, i, max_snapshots=train_truncations[i]\n",
    "        )\n",
    "        Q_train[:, train_boundaries[i]:train_boundaries[i + 1]] = Q_ic\n",
    "        del Q_ic\n",
    "        gc.collect()\n",
    "    \n",
    "    bprint(f\"Combined training data shape: {Q_train.shape}\")\n",
    "    \n",
    "    # Load test data\n",
    "    for i, file_path in enumerate(test_files):\n",
    "        Q_ic = load_and_process_snapshots(\n",
    "            file_path, i, dataset_name=\"Test \", max_snapshots=test_truncations[i]\n",
    "        )\n",
    "        Q_test[:, test_boundaries[i]:test_boundaries[i + 1]] = Q_ic\n",
    "        del Q_ic\n",
    "        gc.collect()\n",
    "    \n",
    "    bprint(f\"Combined test data shape: {Q_test.shape}\")\n",
    "    \n",
    "    # Save boundaries\n",
    "    np.savez(\n",
    "        output_path + \"data_boundaries.npz\",\n",
    "        train_boundaries=train_boundaries,\n",
    "        test_boundaries=test_boundaries,\n",
    "        n_spatial=n_spatial,\n",
    "        train_timesteps=train_timesteps,\n",
    "        test_timesteps=test_timesteps,\n",
    "        truncate_data=truncate_data,\n",
    "        truncate_snapshots=truncate_snapshots if truncate_snapshots else -1,\n",
    "        truncate_time=truncate_time if truncate_time else -1.0\n",
    "    )\n",
    "\n",
    "else:\n",
    "    bprint(\"Skipping data loading (step_1=False, will load POD directly)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae81962-0268-491a-b0ce-1cf8092e48e0",
   "metadata": {},
   "source": [
    "### Step 1.2: Compute POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d78843-108c-4cd1-8720-6fb3a0f9869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLoading POD basis...\u001b[0m\n",
      "  Loaded POD basis from /scratch2/10407/anthony50102/sciml_roms_hasegawa_wakatani/POD_multi_IC.npz\n",
      "  U shape: (131072, 16001), S shape: (16001,)\n"
     ]
    }
   ],
   "source": [
    "if step_1: \n",
    "    # Compute POD basis from combined training data\n",
    "    bprint(\"Computing POD basis from all training trajectories...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    U, S, _ = np.linalg.svd(Q_train, full_matrices=False)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  POD computation completed in {elapsed:.3f} seconds.\")\n",
    "    \n",
    "    # Save POD data\n",
    "    POD_file_multi = output_path + \"POD_multi_IC.npz\"\n",
    "    np.savez(POD_file_multi, S=S, U=U)\n",
    "    print(f\"  Saved POD basis to {POD_file_multi}\")\n",
    "    print(f\"  U shape: {U.shape}, S shape: {S.shape}\")\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Load previous POD basis\n",
    "    bprint(\"Loading POD basis...\")\n",
    "    POD_file_multi = output_path + \"POD_multi_IC.npz\"\n",
    "    POD_multi = np.load(POD_file_multi)\n",
    "    S, U = POD_multi['S'], POD_multi['U']\n",
    "    del POD_multi\n",
    "    gc.collect()\n",
    "    print(f\"  Loaded POD basis from {POD_file_multi}\")\n",
    "    print(f\"  U shape: {U.shape}, S shape: {S.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b79305-8d6a-4ba0-8d83-9686e83fe473",
   "metadata": {},
   "source": [
    "### Step 1.3: Project train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f1e87c-67d3-4779-943b-09299f806814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLoading pre-computed projections...\u001b[0m\n",
      "  Loaded Xhat_train: (16001, 100)\n",
      "  Loaded Xhat_test: (16001, 100)\n"
     ]
    }
   ],
   "source": [
    "if step_1:\n",
    "    # Project training data\n",
    "    bprint(\"Projecting training data...\")\n",
    "    # Use only the modes we need (truncate U to r modes for projection storage)\n",
    "    Ur = U[:, :r]\n",
    "    \n",
    "    Xhat_train = Q_train.T @ Ur  # Shape: (n_time, r) - much smaller!\n",
    "    Xhat_train_file = output_path + \"X_hat_train_multi_IC.npy\"\n",
    "    np.save(Xhat_train_file, Xhat_train)\n",
    "    print(f\"  Saved to {Xhat_train_file}, shape: {Xhat_train.shape}\")\n",
    "    \n",
    "    # Project test data\n",
    "    bprint(\"Projecting test data...\")\n",
    "    Xhat_test = Q_test.T @ Ur\n",
    "    Xhat_test_file = output_path + \"X_hat_test_multi_IC.npy\"\n",
    "    np.save(Xhat_test_file, Xhat_test)\n",
    "    print(f\"  Saved to {Xhat_test_file}, shape: {Xhat_test.shape}\")\n",
    "    \n",
    "    del Ur\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Load pre-computed projections\n",
    "    bprint(\"Loading pre-computed projections...\")\n",
    "    Xhat_train_file = output_path + \"X_hat_train_multi_IC.npy\"\n",
    "    Xhat_test_file = output_path + \"X_hat_test_multi_IC.npy\"\n",
    "    \n",
    "    Xhat_train = np.load(Xhat_train_file)\n",
    "    Xhat_test = np.load(Xhat_test_file)\n",
    "    print(f\"  Loaded Xhat_train: {Xhat_train.shape}\")\n",
    "    print(f\"  Loaded Xhat_test: {Xhat_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e053e9f-c7ef-4a07-a8d0-2cf74aebd30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSkipping IC saving (step_1=False)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if step_1:\n",
    "    # Save initial conditions for later use\n",
    "    bprint(\"Saving initial conditions...\")\n",
    "    \n",
    "    # Load boundaries\n",
    "    boundaries = np.load(output_path + \"data_boundaries.npz\")\n",
    "    train_boundaries = boundaries['train_boundaries']\n",
    "    test_boundaries = boundaries['test_boundaries']\n",
    "    \n",
    "    # Extract ICs from memmap (only first timestep of each trajectory)\n",
    "    train_ICs = np.array([Q_train[:, train_boundaries[i]] for i in range(len(training_files))])\n",
    "    test_ICs = np.array([Q_test[:, test_boundaries[i]] for i in range(len(test_files))])\n",
    "    \n",
    "    # Reduced ICs from projected data\n",
    "    train_ICs_reduced = np.array([Xhat_train[train_boundaries[i], :] for i in range(len(training_files))])\n",
    "    test_ICs_reduced = np.array([Xhat_test[test_boundaries[i], :] for i in range(len(test_files))])\n",
    "    \n",
    "    np.savez(\n",
    "        output_path + \"initial_conditions_multi_IC.npz\",\n",
    "        train_ICs=train_ICs,\n",
    "        test_ICs=test_ICs,\n",
    "        train_ICs_reduced=train_ICs_reduced,\n",
    "        test_ICs_reduced=test_ICs_reduced\n",
    "    )\n",
    "    print(f\"  Saved ICs: train_ICs {train_ICs.shape}, test_ICs {test_ICs.shape}\")\n",
    "    \n",
    "    del train_ICs, test_ICs, train_ICs_reduced, test_ICs_reduced\n",
    "    \n",
    "    # Clean up large memory-mapped arrays - we're done with them\n",
    "    bprint(\"Cleaning up large arrays...\")\n",
    "    del Q_train, Q_test\n",
    "    gc.collect()\n",
    "    \n",
    "    # Optionally remove memmap files to free disk space\n",
    "    # cleanup_memmap(\"Q_train\")\n",
    "    # cleanup_memmap(\"Q_test\")\n",
    "    \n",
    "    bprint(\"Done with Step 1.\")\n",
    "\n",
    "else:\n",
    "    bprint(\"Skipping IC saving (step_1=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd696e-23b2-42da-bd79-0769436eb224",
   "metadata": {},
   "source": [
    "## Step 2: Compute ROM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce9475-1d76-4fb0-bb17-58ca3aff9a38",
   "metadata": {},
   "source": [
    "### Step 2.1: Prep data\n",
    "\n",
    "**Important: Handling Multiple Initial Conditions Correctly**\n",
    "\n",
    "When learning the **state evolution model** ($\\hat{x}_{k+1} = f(\\hat{x}_k)$), we must avoid creating false transitions between trajectories:\n",
    "\n",
    "| Approach | What Happens |\n",
    "|----------|--------------|\n",
    "| ❌ **Wrong**: `X_state = Xhat_all[:-1]`, `Y_state = Xhat_all[1:]` | Creates a fake transition from last state of traj A → first state of traj B |\n",
    "| ✅ **Correct**: Create pairs within each trajectory, then stack | Each `(X_state[i], Y_state[i])` pair is a valid physical transition |\n",
    "\n",
    "For the **output model** ($y = g(\\hat{x})$), simple concatenation is fine because each sample is independent (no temporal dependency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0584c3dc-4e24-46a1-99a5-6ccc5b366542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPrepare the data for learning...\u001b[0m\n",
      "Training data shape: (16001, 100)\n",
      "  Number of training trajectories: 1\n",
      "  Trajectory boundaries: [    0 16001]\n",
      "    Trajectory 1: 16001 timesteps -> 16000 valid pairs\n",
      "\n",
      "  Total valid state pairs: 16000\n",
      "  (Note: This is 1 fewer than naive approach due to excluding trajectory boundaries)\n",
      "\u001b[1mState learning data prepared\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Prepare the data for learning...\")\n",
    "\n",
    "# Ensure we have Xhat_train loaded\n",
    "if 'Xhat_train' not in dir() or Xhat_train is None:\n",
    "    bprint(\"Loading Xhat_train from disk...\")\n",
    "    Xhat_train = np.load(output_path + \"X_hat_train_multi_IC.npy\")\n",
    "\n",
    "print(f\"Training data shape: {Xhat_train.shape}\")\n",
    "\n",
    "# Truncate to r modes if needed (in case we loaded full projection)\n",
    "if Xhat_train.shape[1] > r:\n",
    "    Xhat_train = Xhat_train[:, :r]\n",
    "    print(f\"  Truncated to r={r} modes: {Xhat_train.shape}\")\n",
    "\n",
    "# Load trajectory boundaries to handle multiple ICs correctly\n",
    "boundaries_data = np.load(output_path + \"data_boundaries.npz\")\n",
    "train_boundaries = boundaries_data['train_boundaries']\n",
    "n_train_traj = len(train_boundaries) - 1\n",
    "\n",
    "print(f\"  Number of training trajectories: {n_train_traj}\")\n",
    "print(f\"  Trajectory boundaries: {train_boundaries}\")\n",
    "\n",
    "# CORRECT APPROACH: Create input-output pairs WITHIN each trajectory, then concatenate\n",
    "# This avoids creating false transitions between the end of one trajectory \n",
    "# and the start of the next\n",
    "X_state_list = []\n",
    "Y_state_list = []\n",
    "\n",
    "for traj_idx in range(n_train_traj):\n",
    "    start_idx = train_boundaries[traj_idx]\n",
    "    end_idx = train_boundaries[traj_idx + 1]\n",
    "    \n",
    "    # Extract this trajectory's data\n",
    "    Xhat_traj = Xhat_train[start_idx:end_idx, :]\n",
    "    \n",
    "    # Create valid input-output pairs within this trajectory\n",
    "    X_state_traj = Xhat_traj[:-1, :]  # States k (exclude last)\n",
    "    Y_state_traj = Xhat_traj[1:, :]   # States k+1 (exclude first)\n",
    "    \n",
    "    X_state_list.append(X_state_traj)\n",
    "    Y_state_list.append(Y_state_traj)\n",
    "    \n",
    "    print(f\"    Trajectory {traj_idx + 1}: {Xhat_traj.shape[0]} timesteps -> {X_state_traj.shape[0]} valid pairs\")\n",
    "\n",
    "# Stack all valid pairs (no false transitions between trajectories!)\n",
    "X_state = np.vstack(X_state_list)\n",
    "Y_state = np.vstack(Y_state_list)\n",
    "\n",
    "print(f\"\\n  Total valid state pairs: {X_state.shape[0]}\")\n",
    "print(f\"  (Note: This is {n_train_traj} fewer than naive approach due to excluding trajectory boundaries)\")\n",
    "\n",
    "s = int(r * (r + 1) / 2)\n",
    "d_state = r + s\n",
    "d_out = r + s + 1\n",
    "\n",
    "X_state2 = get_x_sq(X_state)\n",
    "D_state = np.concatenate((X_state, X_state2), axis=1)\n",
    "D_state_2 = D_state.T @ D_state\n",
    "bprint(\"State learning data prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6cba5c-ed6b-4fe0-ae6a-b20c4651dfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPrepare the output learning data\u001b[0m\n",
      "D_out shape: (16001, 5151)\n",
      "D_out_2 condition number: 2.39e+17\n",
      "\u001b[1mDone\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Prepare the output learning data\")\n",
    "\n",
    "# NOTE: For output learning, we use ALL timesteps (not pairs), so simple concatenation is correct\n",
    "# Each row is an independent sample: X_out[k] -> Y_Gamma[k] (no temporal dependency)\n",
    "X_out = Xhat_train\n",
    "K = X_out.shape[0]\n",
    "E = np.ones((K, 1))\n",
    "\n",
    "mean_Xhat = np.mean(X_out, axis=0)\n",
    "Xhat_out = X_out - mean_Xhat[np.newaxis, :]\n",
    "\n",
    "local_min = np.min(X_out)\n",
    "local_max = np.max(X_out)\n",
    "local_scaling = np.maximum(np.abs(local_min), np.abs(local_max))\n",
    "scaling_Xhat = local_scaling\n",
    "\n",
    "Xhat_out /= scaling_Xhat\n",
    "Xhat_out2 = get_x_sq(Xhat_out)\n",
    "\n",
    "D_out = np.concatenate((Xhat_out, Xhat_out2, E), axis=1)\n",
    "D_out_2 = D_out.T @ D_out\n",
    "\n",
    "print(f\"D_out shape: {D_out.shape}\")\n",
    "print(f\"D_out_2 condition number: {np.linalg.cond(D_out_2):.2e}\")\n",
    "bprint(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf5f256-c5c4-4e58-a22a-6fc2b167d03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLoad derived quantities from all training trajectories\u001b[0m\n",
      "\u001b[91m ERROR: Could not open file /work2/10407/anthony50102/frontera/data/hw2d_sim/t600_d256x256_raw/hw2d_sim_step0.025_end1_pts512_c11_k015_N3_nu5e-8_20250315142044_11702_0.h5: variable '/density' has no dimension scale associated with axis 0. \n",
      "Use phony_dims='sort' for sorted naming or phony_dims='access' for per access naming. \u001b[0m\n",
      "  Retrying with phony_dims='sort'...\n",
      "Gamma_n shape: (16001,)\n",
      "Gamma_c shape: (16001,)\n",
      "Y_Gamma shape: (2, 16001)\n",
      "X_out shape (for output learning): (16001, 100)\n",
      "Shape compatibility check: Y_Gamma cols (16001) vs X_out rows (16001)\n",
      "Mean Gamma_n: 0.5901, Std: 0.0416\n",
      "Mean Gamma_c: 0.5843, Std: 0.0354\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "bprint(\"Load derived quantities from all training trajectories\")\n",
    "\n",
    "Gamma_n_list = []\n",
    "Gamma_c_list = []\n",
    "\n",
    "for file_path in training_files:\n",
    "    fh = loader(file_path, ENGINE=ENGINE)\n",
    "    Gamma_n_list.append(fh[\"gamma_n\"].data)\n",
    "    Gamma_c_list.append(fh[\"gamma_c\"].data)\n",
    "\n",
    "# Concatenate all trajectories\n",
    "Gamma_n = np.concatenate(Gamma_n_list)\n",
    "Gamma_c = np.concatenate(Gamma_c_list)\n",
    "\n",
    "mean_Gamma_n_ref = np.mean(Gamma_n)\n",
    "std_Gamma_n_ref = np.std(Gamma_n, ddof=1)\n",
    "\n",
    "mean_Gamma_c_ref = np.mean(Gamma_c)\n",
    "std_Gamma_c_ref = np.std(Gamma_c, ddof=1)\n",
    "\n",
    "Y_Gamma = np.vstack((Gamma_n, Gamma_c))\n",
    "\n",
    "print(f\"Gamma_n shape: {Gamma_n.shape}\")\n",
    "print(f\"Gamma_c shape: {Gamma_c.shape}\")\n",
    "print(f\"Y_Gamma shape: {Y_Gamma.shape}\")\n",
    "print(f\"X_out shape (for output learning): {X_out.shape}\")\n",
    "print(f\"Shape compatibility check: Y_Gamma cols ({Y_Gamma.shape[1]}) vs X_out rows ({X_out.shape[0]})\")\n",
    "\n",
    "if Y_Gamma.shape[1] != X_out.shape[0]:\n",
    "    raise ValueError(f\"Shape mismatch: Y_Gamma has {Y_Gamma.shape[1]} columns but X_out has {X_out.shape[0]} rows\")\n",
    "\n",
    "print(f\"Mean Gamma_n: {mean_Gamma_n_ref:.4f}, Std: {std_Gamma_n_ref:.4f}\")\n",
    "print(f\"Mean Gamma_c: {mean_Gamma_c_ref:.4f}, Std: {std_Gamma_c_ref:.4f}\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6649b-695e-4b1f-84f7-c5b76dd289cb",
   "metadata": {},
   "source": [
    "### Step 2.2: Model Regularization Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f02c6bee-cbae-4791-9ef1-825efc17e17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mModel selection: Threshold-based\u001b[0m\n",
      "  Mean error threshold: 5.0%\n",
      "  Std error threshold: 30.0%\n"
     ]
    }
   ],
   "source": [
    "# Initialize model collector based on selection method\n",
    "if model_selection_method == \"top_k\":\n",
    "    model_collector = TopKModels(k=num_top_models)\n",
    "    bprint(f\"Model selection: Top-{num_top_models} by total error\")\n",
    "elif model_selection_method == \"threshold\":\n",
    "    model_collector = ThresholdModels(\n",
    "        threshold_mean=threshold_mean_error,\n",
    "        threshold_std=threshold_std_error\n",
    "    )\n",
    "    bprint(f\"Model selection: Threshold-based\")\n",
    "    print(f\"  Mean error threshold: {threshold_mean_error:.1%}\")\n",
    "    print(f\"  Std error threshold: {threshold_std_error:.1%}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model_selection_method: {model_selection_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5469829-cce8-4cd9-8766-f44b75ee54bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSkipping model search (step_2=False)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if step_2:\n",
    "    bprint(\"BEGIN HYPERPARAMETER SWEEP\")\n",
    "    print(f\"  State regularization: {len(ridge_alf_lin_all)} x {len(ridge_alf_quad_all)} combinations\")\n",
    "    print(f\"  Output regularization: {len(gamma_reg_lin)} x {len(gamma_reg_quad)} combinations\")\n",
    "    \n",
    "    n_total_combinations = (len(ridge_alf_lin_all) * len(ridge_alf_quad_all) * \n",
    "                           len(gamma_reg_lin) * len(gamma_reg_quad))\n",
    "    print(f\"  Total combinations to evaluate: {n_total_combinations}\\n\")\n",
    "    \n",
    "    sweep_start_time = time.time()\n",
    "    n_evaluated = 0\n",
    "    n_nan_models = 0\n",
    "    n_accepted = 0\n",
    "    best_error_so_far = float('inf')\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=n_total_combinations, \n",
    "                desc=\"Evaluating models\",\n",
    "                ncols=100,\n",
    "                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
    "    \n",
    "    for alpha_state_lin in ridge_alf_lin_all:\n",
    "        for alpha_state_quad in ridge_alf_quad_all:\n",
    "            # Construct and solve regularized state operator learning problem\n",
    "            regg = np.zeros(d_state)\n",
    "            regg[:r] = alpha_state_lin\n",
    "            regg[r:r + s] = alpha_state_quad\n",
    "            regularizer = np.diag(regg)\n",
    "            D_state_reg = D_state_2 + regularizer\n",
    "\n",
    "            O = np.linalg.solve(D_state_reg, np.dot(D_state.T, Y_state)).T\n",
    "\n",
    "            A = O[:, :r]\n",
    "            F = O[:, r:r + s]\n",
    "            \n",
    "            # Define state evolution function (closure captures current A, F)\n",
    "            A_current, F_current = A.copy(), F.copy()\n",
    "            f = lambda x, A=A_current, F=F_current: np.dot(A, x) + np.dot(F, get_x_sq(x))\n",
    "\n",
    "            # Integrate learned model forward in time\n",
    "            u0 = X_state[0, :]\n",
    "            is_nan, Xhat_pred = solve_opinf_difference_model(u0, n_steps, f)\n",
    "            \n",
    "            if is_nan:\n",
    "                n_nan_models += 1\n",
    "                # Still update progress for skipped combinations\n",
    "                pbar.update(len(gamma_reg_lin) * len(gamma_reg_quad))\n",
    "                continue\n",
    "                \n",
    "            X_OpInf_full = Xhat_pred.T\n",
    "            \n",
    "            # Prepare predicted states for output operator learning\n",
    "            Xhat_OpInf_scaled = (X_OpInf_full - mean_Xhat[np.newaxis, :]) / scaling_Xhat\n",
    "            Xhat_2_OpInf = get_x_sq(Xhat_OpInf_scaled)\n",
    "            \n",
    "            # Sweep over output operator regularization\n",
    "            for alpha_out_lin in gamma_reg_lin:\n",
    "                for alpha_out_quad in gamma_reg_quad:\n",
    "                    n_evaluated += 1\n",
    "                    \n",
    "                    # Construct and solve regularized output operator learning problem\n",
    "                    regg_out = np.zeros(d_out)\n",
    "                    regg_out[:r] = alpha_out_lin\n",
    "                    regg_out[r:r + s] = alpha_out_quad\n",
    "                    regg_out[r + s:] = alpha_out_lin\n",
    "                    regularizer_out = np.diag(regg_out)\n",
    "                    D_out_reg = D_out_2 + regularizer_out\n",
    "\n",
    "                    O_out = np.linalg.solve(D_out_reg, np.dot(D_out.T, Y_Gamma.T)).T\n",
    "\n",
    "                    C = O_out[:, :r]\n",
    "                    G = O_out[:, r:r + s]\n",
    "                    c = O_out[:, r + s]\n",
    "\n",
    "                    # Compute output predictions\n",
    "                    Y_OpInf = (\n",
    "                        C @ Xhat_OpInf_scaled.T\n",
    "                        + G @ Xhat_2_OpInf.T\n",
    "                        + c[:, np.newaxis]\n",
    "                    )\n",
    "\n",
    "                    ts_Gamma_n = Y_OpInf[0, :]\n",
    "                    ts_Gamma_c = Y_OpInf[1, :]\n",
    "                    \n",
    "                    # Compute statistical error metrics on training portion\n",
    "                    mean_Gamma_n_OpInf = np.mean(ts_Gamma_n[:training_end])\n",
    "                    std_Gamma_n_OpInf = np.std(ts_Gamma_n[:training_end], ddof=1)\n",
    "                    mean_Gamma_c_OpInf = np.mean(ts_Gamma_c[:training_end])\n",
    "                    std_Gamma_c_OpInf = np.std(ts_Gamma_c[:training_end], ddof=1)\n",
    "                    \n",
    "                    # Relative errors\n",
    "                    mean_err_Gamma_n = np.abs(mean_Gamma_n_ref - mean_Gamma_n_OpInf) / np.abs(mean_Gamma_n_ref)\n",
    "                    std_err_Gamma_n = np.abs(std_Gamma_n_ref - std_Gamma_n_OpInf) / std_Gamma_n_ref\n",
    "                    mean_err_Gamma_c = np.abs(mean_Gamma_c_ref - mean_Gamma_c_OpInf) / np.abs(mean_Gamma_c_ref)\n",
    "                    std_err_Gamma_c = np.abs(std_Gamma_c_ref - std_Gamma_c_OpInf) / std_Gamma_c_ref\n",
    "                    \n",
    "                    # Aggregate error metric\n",
    "                    total_error = (mean_err_Gamma_n + std_err_Gamma_n + \n",
    "                                   mean_err_Gamma_c + std_err_Gamma_c)\n",
    "\n",
    "                    # Update best error tracker\n",
    "                    if total_error < best_error_so_far:\n",
    "                        best_error_so_far = total_error\n",
    "                        pbar.set_postfix({'best_err': f'{best_error_so_far:.4e}', \n",
    "                                         'NaNs': n_nan_models}, refresh=False)\n",
    "\n",
    "                    # Store model with all metadata\n",
    "                    model = {\n",
    "                        'A': A.copy(),\n",
    "                        'F': F.copy(),\n",
    "                        'C': C.copy(),\n",
    "                        'G': G.copy(),\n",
    "                        'c': c.copy(),\n",
    "                        'total_error': total_error,\n",
    "                        'mean_err_Gamma_n': mean_err_Gamma_n,\n",
    "                        'std_err_Gamma_n': std_err_Gamma_n,\n",
    "                        'mean_err_Gamma_c': mean_err_Gamma_c,\n",
    "                        'std_err_Gamma_c': std_err_Gamma_c,\n",
    "                        'alpha_state_lin': alpha_state_lin,\n",
    "                        'alpha_state_quad': alpha_state_quad,\n",
    "                        'alpha_out_lin': alpha_out_lin,\n",
    "                        'alpha_out_quad': alpha_out_quad\n",
    "                    }\n",
    "\n",
    "                    # Add to collector based on selection method\n",
    "                    if model_selection_method == \"top_k\":\n",
    "                        model_collector.add(score=total_error, model=model)\n",
    "                    elif model_selection_method == \"threshold\":\n",
    "                        if model_collector.add(model):\n",
    "                            n_accepted += 1\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    sweep_elapsed = time.time() - sweep_start_time\n",
    "    \n",
    "    print()  # Blank line after progress bar\n",
    "    bprint(\"HYPERPARAMETER SWEEP COMPLETE\")\n",
    "    print(f\"  Total time: {sweep_elapsed:.1f} seconds ({sweep_elapsed/60:.1f} minutes)\")\n",
    "    print(f\"  Models evaluated: {n_evaluated}\")\n",
    "    print(f\"  Models with NaN: {n_nan_models}\")\n",
    "    \n",
    "    # Retrieve selected models\n",
    "    best_models = model_collector.get_best()\n",
    "    \n",
    "    if model_selection_method == \"top_k\":\n",
    "        print(f\"  Models selected (top-{num_top_models}): {len(best_models)}\")\n",
    "    elif model_selection_method == \"threshold\":\n",
    "        print(f\"  Models meeting threshold criteria: {len(best_models)}\")\n",
    "    \n",
    "    if len(best_models) > 0:\n",
    "        print(f\"\\n  Best model total error: {best_models[0][0]:.6e}\")\n",
    "        print(f\"  Worst selected model total error: {best_models[-1][0]:.6e}\")\n",
    "        \n",
    "        # Summary table of selected models\n",
    "        print(f\"\\n  {'='*70}\")\n",
    "        print(f\"  {'Model':>6} | {'Total Err':>10} | {'Mean Γn':>8} | {'Std Γn':>8} | {'Mean Γc':>8} | {'Std Γc':>8}\")\n",
    "        print(f\"  {'-'*70}\")\n",
    "        for i, (score, model) in enumerate(best_models[:10]):  # Show top 10\n",
    "            print(f\"  {i+1:>6} | {score:>10.4e} | {model['mean_err_Gamma_n']:>8.4f} | \"\n",
    "                  f\"{model['std_err_Gamma_n']:>8.4f} | {model['mean_err_Gamma_c']:>8.4f} | \"\n",
    "                  f\"{model['std_err_Gamma_c']:>8.4f}\")\n",
    "        if len(best_models) > 10:\n",
    "            print(f\"  {'...'}\")\n",
    "            print(f\"  (showing 10 of {len(best_models)} models)\")\n",
    "        print(f\"  {'='*70}\")\n",
    "    else:\n",
    "        bprint(\"WARNING: No models met selection criteria!\")\n",
    "        if model_selection_method == \"threshold\":\n",
    "            print(\"  Consider relaxing threshold_mean_error or threshold_std_error\")\n",
    "    \n",
    "else:\n",
    "    bprint(\"Skipping model search (step_2=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2fd9b76-3d5e-45a9-a61a-640f54d4a459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLoading pre-computed ensemble models...\u001b[0m\n",
      "  Using legacy file format: /scratch2/10407/anthony50102/sciml_roms_hasegawa_wakatani/ensemble_models_r100_k20.npz\n",
      "  Loaded 20 models from: /scratch2/10407/anthony50102/sciml_roms_hasegawa_wakatani/ensemble_models_r100_k20.npz\n",
      "  Selection method used: top_k\n",
      "  Best model total error: 1.730914e-01\n"
     ]
    }
   ],
   "source": [
    "if step_2:\n",
    "    bprint(\"Saving ensemble models...\")\n",
    "    \n",
    "    if best_models is not None and len(best_models) > 0:\n",
    "        # Construct filename with selection method info\n",
    "        if model_selection_method == \"top_k\":\n",
    "            model_filename = f\"ensemble_models_r{r}_topk{len(best_models)}.npz\"\n",
    "        else:\n",
    "            model_filename = f\"ensemble_models_r{r}_thresh{len(best_models)}.npz\"\n",
    "        \n",
    "        # Build ensemble data dictionary\n",
    "        ensemble_data = {\n",
    "            'num_models': len(best_models),\n",
    "            'selection_method': model_selection_method,\n",
    "            'r': r\n",
    "        }\n",
    "        \n",
    "        # Store threshold parameters if using threshold method\n",
    "        if model_selection_method == \"threshold\":\n",
    "            ensemble_data['threshold_mean_error'] = threshold_mean_error\n",
    "            ensemble_data['threshold_std_error'] = threshold_std_error\n",
    "        else:\n",
    "            ensemble_data['num_top_models'] = num_top_models\n",
    "        \n",
    "        # Save each model's operators and hyperparameters\n",
    "        for i, (score, model) in enumerate(best_models):\n",
    "            prefix = f'model_{i}_'\n",
    "            ensemble_data[prefix + 'A'] = model['A']\n",
    "            ensemble_data[prefix + 'F'] = model['F']\n",
    "            ensemble_data[prefix + 'C'] = model['C']\n",
    "            ensemble_data[prefix + 'G'] = model['G']\n",
    "            ensemble_data[prefix + 'c'] = model['c']\n",
    "            ensemble_data[prefix + 'alpha_state_lin'] = model['alpha_state_lin']\n",
    "            ensemble_data[prefix + 'alpha_state_quad'] = model['alpha_state_quad']\n",
    "            ensemble_data[prefix + 'alpha_out_lin'] = model['alpha_out_lin']\n",
    "            ensemble_data[prefix + 'alpha_out_quad'] = model['alpha_out_quad']\n",
    "            ensemble_data[prefix + 'total_error'] = model['total_error']\n",
    "            ensemble_data[prefix + 'mean_err_Gamma_n'] = model['mean_err_Gamma_n']\n",
    "            ensemble_data[prefix + 'std_err_Gamma_n'] = model['std_err_Gamma_n']\n",
    "            ensemble_data[prefix + 'mean_err_Gamma_c'] = model['mean_err_Gamma_c']\n",
    "            ensemble_data[prefix + 'std_err_Gamma_c'] = model['std_err_Gamma_c']\n",
    "        \n",
    "        np.savez(output_path + model_filename, **ensemble_data)\n",
    "        bprint(f\"Ensemble saved: {output_path}{model_filename}\")\n",
    "        print(f\"  Selection method: {model_selection_method}\")\n",
    "        print(f\"  Number of models: {len(best_models)}\")\n",
    "        \n",
    "    else:\n",
    "        bprint(\"WARNING: No valid models found during sweep!\")\n",
    "        \n",
    "else:\n",
    "    bprint(\"Loading pre-computed ensemble models...\")\n",
    "    \n",
    "    # Try to find existing ensemble file\n",
    "    # First try the current selection method, then try alternatives\n",
    "    if model_selection_method == \"top_k\":\n",
    "        primary_file = output_path + f\"ensemble_models_r{r}_topk{num_top_models}.npz\"\n",
    "        alt_pattern = f\"ensemble_models_r{r}_topk*.npz\"\n",
    "    else:\n",
    "        primary_file = output_path + f\"ensemble_models_r{r}_thresh*.npz\"\n",
    "        alt_pattern = f\"ensemble_models_r{r}_thresh*.npz\"\n",
    "    \n",
    "    # Also check legacy filename format\n",
    "    legacy_file = output_path + f\"ensemble_models_r{r}_k{num_top_models}.npz\"\n",
    "    \n",
    "    model_file = None\n",
    "    if os.path.exists(primary_file):\n",
    "        model_file = primary_file\n",
    "    elif os.path.exists(legacy_file):\n",
    "        model_file = legacy_file\n",
    "        print(f\"  Using legacy file format: {legacy_file}\")\n",
    "    else:\n",
    "        # Search for any matching ensemble file\n",
    "        import glob\n",
    "        candidates = glob.glob(output_path + f\"ensemble_models_r{r}_*.npz\")\n",
    "        if candidates:\n",
    "            model_file = candidates[0]\n",
    "            print(f\"  Found ensemble file: {model_file}\")\n",
    "    \n",
    "    if model_file is None or not os.path.exists(model_file):\n",
    "        bprint(f\"ERROR: No ensemble model file found for r={r}\")\n",
    "        best_models = None\n",
    "    else:\n",
    "        ensemble_data = np.load(model_file, allow_pickle=True)\n",
    "        num_loaded = int(ensemble_data['num_models'])\n",
    "        \n",
    "        # Read selection method metadata if available\n",
    "        loaded_selection_method = str(ensemble_data.get('selection_method', 'top_k'))\n",
    "        \n",
    "        best_models = []\n",
    "        for i in range(num_loaded):\n",
    "            prefix = f'model_{i}_'\n",
    "            model = {\n",
    "                'A': ensemble_data[prefix + 'A'],\n",
    "                'F': ensemble_data[prefix + 'F'],\n",
    "                'C': ensemble_data[prefix + 'C'],\n",
    "                'G': ensemble_data[prefix + 'G'],\n",
    "                'c': ensemble_data[prefix + 'c'],\n",
    "                'total_error': float(ensemble_data[prefix + 'total_error']),\n",
    "                'mean_err_Gamma_n': float(ensemble_data[prefix + 'mean_err_Gamma_n']),\n",
    "                'std_err_Gamma_n': float(ensemble_data[prefix + 'std_err_Gamma_n']),\n",
    "                'mean_err_Gamma_c': float(ensemble_data[prefix + 'mean_err_Gamma_c']),\n",
    "                'std_err_Gamma_c': float(ensemble_data[prefix + 'std_err_Gamma_c']),\n",
    "                'alpha_state_lin': float(ensemble_data[prefix + 'alpha_state_lin']),\n",
    "                'alpha_state_quad': float(ensemble_data[prefix + 'alpha_state_quad']),\n",
    "                'alpha_out_lin': float(ensemble_data[prefix + 'alpha_out_lin']),\n",
    "                'alpha_out_quad': float(ensemble_data[prefix + 'alpha_out_quad'])\n",
    "            }\n",
    "            score = model['total_error']\n",
    "            best_models.append((score, model))\n",
    "        \n",
    "        print(f\"  Loaded {num_loaded} models from: {model_file}\")\n",
    "        print(f\"  Selection method used: {loaded_selection_method}\")\n",
    "        print(f\"  Best model total error: {best_models[0][0]:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e6c08-a20e-4cbc-bdf1-a01d7fbd27a9",
   "metadata": {},
   "source": [
    "## Step 3: Make Predictions with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df11bc1d-c1f1-48da-8afc-e04a49ca06e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mComputing ensemble predictions with 20 models...\u001b[0m\n",
      "  Number of training trajectories: 1\n",
      "  Number of test trajectories: 1\n",
      "\u001b[1m\n",
      "Processing TRAINING trajectories...\u001b[0m\n",
      "\n",
      "  Training trajectory 1/1\n",
      "\u001b[1m\n",
      "Processing TEST trajectories...\u001b[0m\n",
      "\n",
      "  Test trajectory 1/1\n",
      "\u001b[1m\n",
      "Saving ensemble trajectory predictions...\u001b[0m\n",
      "\u001b[1mSaved to: /scratch2/10407/anthony50102/sciml_roms_hasegawa_wakatani/ensemble_trajectory_predictions_r100_k20.npz\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if best_models is not None and len(best_models) > 0:\n",
    "    bprint(f\"Computing ensemble predictions with {len(best_models)} models...\")\n",
    "    \n",
    "    # Load initial conditions and boundaries\n",
    "    IC_data = np.load(output_path + \"initial_conditions_multi_IC.npz\")\n",
    "    boundaries_data = np.load(output_path + \"data_boundaries.npz\")\n",
    "    \n",
    "    train_ICs_reduced = IC_data['train_ICs_reduced']  # Shape: (n_train_traj, r)\n",
    "    test_ICs_reduced = IC_data['test_ICs_reduced']    # Shape: (n_test_traj, r)\n",
    "    train_boundaries = boundaries_data['train_boundaries']\n",
    "    test_boundaries = boundaries_data['test_boundaries']\n",
    "    \n",
    "    n_train_traj = len(train_boundaries) - 1\n",
    "    n_test_traj = len(test_boundaries) - 1\n",
    "    \n",
    "    print(f\"  Number of training trajectories: {n_train_traj}\")\n",
    "    print(f\"  Number of test trajectories: {n_test_traj}\")\n",
    "    \n",
    "    # Storage for ensemble predictions\n",
    "    # For each trajectory, we'll store predictions from all models\n",
    "    train_predictions = {\n",
    "        'Gamma_n': [],  # List of [n_models, n_timesteps] arrays, one per trajectory\n",
    "        'Gamma_c': [],\n",
    "        'X_OpInf': []\n",
    "    }\n",
    "    \n",
    "    test_predictions = {\n",
    "        'Gamma_n': [],\n",
    "        'Gamma_c': [],\n",
    "        'X_OpInf': []\n",
    "    }\n",
    "    \n",
    "    ###########################\n",
    "    # TRAINING TRAJECTORIES\n",
    "    ###########################\n",
    "    bprint(\"\\nProcessing TRAINING trajectories...\")\n",
    "    for traj_idx in range(n_train_traj):\n",
    "        print(f\"\\n  Training trajectory {traj_idx + 1}/{n_train_traj}\")\n",
    "        \n",
    "        # Get this trajectory's length\n",
    "        traj_length = train_boundaries[traj_idx + 1] - train_boundaries[traj_idx]\n",
    "        \n",
    "        # Storage for all models' predictions for this trajectory\n",
    "        traj_Gamma_n_preds = []\n",
    "        traj_Gamma_c_preds = []\n",
    "        traj_X_OpInf_preds = []\n",
    "        \n",
    "        # Initial condition for this trajectory\n",
    "        u0 = train_ICs_reduced[traj_idx, :]\n",
    "        \n",
    "        # Run each model\n",
    "        for model_idx, (score, model) in enumerate(best_models):\n",
    "            # Extract operators\n",
    "            A_model = model['A']\n",
    "            F_model = model['F']\n",
    "            C_model = model['C']\n",
    "            G_model = model['G']\n",
    "            c_model = model['c']\n",
    "            \n",
    "            # State evolution function\n",
    "            f = lambda x: np.dot(A_model, x) + np.dot(F_model, get_x_sq(x))\n",
    "            \n",
    "            # Solve for this trajectory length\n",
    "            is_nan, Xhat_pred = solve_opinf_difference_model(u0, traj_length, f)\n",
    "            \n",
    "            if is_nan:\n",
    "                print(f\"    WARNING: NaN in model {model_idx + 1}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            X_OpInf_full = Xhat_pred.T  # Shape: (traj_length, r)\n",
    "            \n",
    "            # Apply output operators\n",
    "            Xhat_OpInf_scaled = (X_OpInf_full - mean_Xhat[np.newaxis, :]) / scaling_Xhat\n",
    "            Xhat_2_OpInf = get_x_sq(Xhat_OpInf_scaled)\n",
    "            \n",
    "            Y_OpInf = (\n",
    "                C_model @ Xhat_OpInf_scaled.T\n",
    "                + G_model @ Xhat_2_OpInf.T\n",
    "                + c_model[:, np.newaxis]\n",
    "            )\n",
    "            \n",
    "            traj_Gamma_n_preds.append(Y_OpInf[0, :])\n",
    "            traj_Gamma_c_preds.append(Y_OpInf[1, :])\n",
    "            traj_X_OpInf_preds.append(X_OpInf_full)\n",
    "        \n",
    "        # Convert to arrays\n",
    "        train_predictions['Gamma_n'].append(np.array(traj_Gamma_n_preds))  # Shape: (n_models, traj_length)\n",
    "        train_predictions['Gamma_c'].append(np.array(traj_Gamma_c_preds))\n",
    "        train_predictions['X_OpInf'].append(np.array(traj_X_OpInf_preds))  # Shape: (n_models, traj_length, r)\n",
    "    \n",
    "    ###########################\n",
    "    # TEST TRAJECTORIES\n",
    "    ###########################\n",
    "    bprint(\"\\nProcessing TEST trajectories...\")\n",
    "    for traj_idx in range(n_test_traj):\n",
    "        print(f\"\\n  Test trajectory {traj_idx + 1}/{n_test_traj}\")\n",
    "        \n",
    "        # Get this trajectory's length\n",
    "        traj_length = test_boundaries[traj_idx + 1] - test_boundaries[traj_idx]\n",
    "        \n",
    "        # Storage for all models' predictions for this trajectory\n",
    "        traj_Gamma_n_preds = []\n",
    "        traj_Gamma_c_preds = []\n",
    "        traj_X_OpInf_preds = []\n",
    "        \n",
    "        # Initial condition for this trajectory\n",
    "        u0 = test_ICs_reduced[traj_idx, :]\n",
    "        \n",
    "        # Run each model\n",
    "        for model_idx, (score, model) in enumerate(best_models):\n",
    "            # Extract operators\n",
    "            A_model = model['A']\n",
    "            F_model = model['F']\n",
    "            C_model = model['C']\n",
    "            G_model = model['G']\n",
    "            c_model = model['c']\n",
    "            \n",
    "            # State evolution function\n",
    "            f = lambda x: np.dot(A_model, x) + np.dot(F_model, get_x_sq(x))\n",
    "            \n",
    "            # Solve for this trajectory length\n",
    "            is_nan, Xhat_pred = solve_opinf_difference_model(u0, traj_length, f)\n",
    "            \n",
    "            if is_nan:\n",
    "                print(f\"    WARNING: NaN in model {model_idx + 1}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            X_OpInf_full = Xhat_pred.T  # Shape: (traj_length, r)\n",
    "            \n",
    "            # Apply output operators\n",
    "            Xhat_OpInf_scaled = (X_OpInf_full - mean_Xhat[np.newaxis, :]) / scaling_Xhat\n",
    "            Xhat_2_OpInf = get_x_sq(Xhat_OpInf_scaled)\n",
    "            \n",
    "            Y_OpInf = (\n",
    "                C_model @ Xhat_OpInf_scaled.T\n",
    "                + G_model @ Xhat_2_OpInf.T\n",
    "                + c_model[:, np.newaxis]\n",
    "            )\n",
    "            \n",
    "            traj_Gamma_n_preds.append(Y_OpInf[0, :])\n",
    "            traj_Gamma_c_preds.append(Y_OpInf[1, :])\n",
    "            traj_X_OpInf_preds.append(X_OpInf_full)\n",
    "        \n",
    "        # Convert to arrays\n",
    "        test_predictions['Gamma_n'].append(np.array(traj_Gamma_n_preds))\n",
    "        test_predictions['Gamma_c'].append(np.array(traj_Gamma_c_preds))\n",
    "        test_predictions['X_OpInf'].append(np.array(traj_X_OpInf_preds))\n",
    "    \n",
    "    ###########################\n",
    "    # Save predictions\n",
    "    ###########################\n",
    "    bprint(\"\\nSaving ensemble trajectory predictions...\")\n",
    "    \n",
    "    save_dict = {\n",
    "        'n_train_traj': n_train_traj,\n",
    "        'n_test_traj': n_test_traj,\n",
    "        'num_models_used': len(best_models),\n",
    "        'train_boundaries': train_boundaries,\n",
    "        'test_boundaries': test_boundaries\n",
    "    }\n",
    "    \n",
    "    # Save each trajectory's predictions\n",
    "    for i in range(n_train_traj):\n",
    "        save_dict[f'train_traj_{i}_Gamma_n'] = train_predictions['Gamma_n'][i]\n",
    "        save_dict[f'train_traj_{i}_Gamma_c'] = train_predictions['Gamma_c'][i]\n",
    "        save_dict[f'train_traj_{i}_X_OpInf'] = train_predictions['X_OpInf'][i]\n",
    "    \n",
    "    for i in range(n_test_traj):\n",
    "        save_dict[f'test_traj_{i}_Gamma_n'] = test_predictions['Gamma_n'][i]\n",
    "        save_dict[f'test_traj_{i}_Gamma_c'] = test_predictions['Gamma_c'][i]\n",
    "        save_dict[f'test_traj_{i}_X_OpInf'] = test_predictions['X_OpInf'][i]\n",
    "    \n",
    "    np.savez(\n",
    "        output_path + f\"ensemble_trajectory_predictions_r{r}_k{len(best_models)}.npz\",\n",
    "        **save_dict\n",
    "    )\n",
    "    \n",
    "    bprint(f\"Saved to: {output_path}ensemble_trajectory_predictions_r{r}_k{len(best_models)}.npz\")\n",
    "    \n",
    "else:\n",
    "    bprint(\"WARNING: No valid models available for ensemble predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30e424ac-fa48-4fde-bcbd-b529c79edd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLoading ensemble trajectory predictions...\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(best_models)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[43mnum_models\u001b[49m\n\u001b[1;32m     10\u001b[0m pred_file \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(output_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensemble_trajectory_predictions_r\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m n_train_traj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(pred_file[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_train_traj\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_models' is not defined"
     ]
    }
   ],
   "source": [
    "### Load the ensemble trajectory predictions\n",
    "bprint(\"Loading ensemble trajectory predictions...\")\n",
    "\n",
    "# Determine number of models to load\n",
    "if step_2 and best_models is not None:\n",
    "    k = len(best_models)\n",
    "else:\n",
    "    k = num_models\n",
    "\n",
    "pred_file = np.load(output_path + f\"ensemble_trajectory_predictions_r{r}_k{k}.npz\", allow_pickle=True)\n",
    "\n",
    "n_train_traj = int(pred_file['n_train_traj'])\n",
    "n_test_traj = int(pred_file['n_test_traj'])\n",
    "num_models_used = int(pred_file['num_models_used'])\n",
    "train_boundaries = pred_file['train_boundaries']\n",
    "test_boundaries = pred_file['test_boundaries']\n",
    "\n",
    "print(f\"\\nLoaded ensemble predictions:\")\n",
    "print(f\"  Number of models in ensemble: {num_models_used}\")\n",
    "print(f\"  Number of training trajectories: {n_train_traj}\")\n",
    "print(f\"  Number of test trajectories: {n_test_traj}\")\n",
    "\n",
    "# Load predictions for each trajectory\n",
    "train_predictions = {'Gamma_n': [], 'Gamma_c': [], 'X_OpInf': []}\n",
    "test_predictions = {'Gamma_n': [], 'Gamma_c': [], 'X_OpInf': []}\n",
    "\n",
    "for i in range(n_train_traj):\n",
    "    train_predictions['Gamma_n'].append(pred_file[f'train_traj_{i}_Gamma_n'])\n",
    "    train_predictions['Gamma_c'].append(pred_file[f'train_traj_{i}_Gamma_c'])\n",
    "    train_predictions['X_OpInf'].append(pred_file[f'train_traj_{i}_X_OpInf'])\n",
    "    print(f\"  Train traj {i}: Gamma_n shape {train_predictions['Gamma_n'][i].shape}\")\n",
    "\n",
    "for i in range(n_test_traj):\n",
    "    test_predictions['Gamma_n'].append(pred_file[f'test_traj_{i}_Gamma_n'])\n",
    "    test_predictions['Gamma_c'].append(pred_file[f'test_traj_{i}_Gamma_c'])\n",
    "    test_predictions['X_OpInf'].append(pred_file[f'test_traj_{i}_X_OpInf'])\n",
    "    print(f\"  Test traj {i}: Gamma_n shape {test_predictions['Gamma_n'][i].shape}\")\n",
    "\n",
    "# Load ground truth\n",
    "bprint(\"\\nLoading ground truth trajectories...\")\n",
    "\n",
    "train_truth = {'Gamma_n': [], 'Gamma_c': []}\n",
    "for i, file_path in enumerate(training_files):\n",
    "    fh = xr.open_dataset(file_path, engine=ENGINE, phony_dims=\"sort\")\n",
    "    train_truth['Gamma_n'].append(fh[\"gamma_n\"].data)\n",
    "    train_truth['Gamma_c'].append(fh[\"gamma_c\"].data)\n",
    "    fh.close()\n",
    "    print(f\"  Train truth {i}: {train_truth['Gamma_n'][i].shape}\")\n",
    "\n",
    "test_truth = {'Gamma_n': [], 'Gamma_c': []}\n",
    "for i, file_path in enumerate(test_files):\n",
    "    fh = xr.open_dataset(file_path, engine=ENGINE, phony_dims=\"sort\")\n",
    "    test_truth['Gamma_n'].append(fh[\"gamma_n\"].data)\n",
    "    test_truth['Gamma_c'].append(fh[\"gamma_c\"].data)\n",
    "    fh.close()\n",
    "    print(f\"  Test truth {i}: {test_truth['Gamma_n'][i].shape}\")\n",
    "\n",
    "bprint(\"Data loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52432fc3-69d7-4c89-ab29-fbd2bccaa9f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Plot ensemble predictions vs ground truth for each trajectory\n",
    "print(\"Creating ensemble prediction plots...\")\n",
    "\n",
    "# Compute ensemble statistics for each trajectory\n",
    "def compute_ensemble_stats(predictions_list):\n",
    "    \"\"\"Compute mean and std across models for a list of trajectory predictions\"\"\"\n",
    "    means = []\n",
    "    stds = []\n",
    "    for traj_preds in predictions_list:\n",
    "        # traj_preds shape: (n_models, n_timesteps)\n",
    "        means.append(np.mean(traj_preds, axis=0))\n",
    "        stds.append(np.std(traj_preds, axis=0))\n",
    "    return means, stds\n",
    "\n",
    "train_Gamma_n_mean, train_Gamma_n_std = compute_ensemble_stats(train_predictions['Gamma_n'])\n",
    "train_Gamma_c_mean, train_Gamma_c_std = compute_ensemble_stats(train_predictions['Gamma_c'])\n",
    "test_Gamma_n_mean, test_Gamma_n_std = compute_ensemble_stats(test_predictions['Gamma_n'])\n",
    "test_Gamma_c_mean, test_Gamma_c_std = compute_ensemble_stats(test_predictions['Gamma_c'])\n",
    "\n",
    "###########################\n",
    "# PLOT 1: Training Trajectories\n",
    "###########################\n",
    "fig, axes = plt.subplots(n_train_traj, 2, figsize=(16, 4*n_train_traj))\n",
    "if n_train_traj == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for traj_idx in range(n_train_traj):\n",
    "    time_steps = np.arange(len(train_Gamma_n_mean[traj_idx]))\n",
    "    \n",
    "    # Gamma_n\n",
    "    ax = axes[traj_idx, 0]\n",
    "    \n",
    "    # Ensemble mean\n",
    "    ax.plot(time_steps, train_Gamma_n_mean[traj_idx], 'b-', linewidth=2, label='Ensemble mean')\n",
    "    \n",
    "    # Uncertainty band\n",
    "    ax.fill_between(time_steps,\n",
    "                    train_Gamma_n_mean[traj_idx] - train_Gamma_n_std[traj_idx],\n",
    "                    train_Gamma_n_mean[traj_idx] + train_Gamma_n_std[traj_idx],\n",
    "                    alpha=0.3, color='blue', label='±1 std')\n",
    "    \n",
    "    # Ground truth\n",
    "    ax.plot(time_steps, train_truth['Gamma_n'][traj_idx], 'k-', linewidth=1.5, \n",
    "            label='Ground truth', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Time Step', fontsize=11)\n",
    "    ax.set_ylabel('Gamma_n', fontsize=11)\n",
    "    ax.set_title(f'Training Trajectory {traj_idx + 1}: Gamma_n', fontsize=12)\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    \n",
    "    # Gamma_c\n",
    "    ax = axes[traj_idx, 1]\n",
    "    \n",
    "    # Ensemble mean\n",
    "    ax.plot(time_steps, train_Gamma_c_mean[traj_idx], 'r-', linewidth=2, label='Ensemble mean')\n",
    "    \n",
    "    # Uncertainty band\n",
    "    ax.fill_between(time_steps,\n",
    "                    train_Gamma_c_mean[traj_idx] - train_Gamma_c_std[traj_idx],\n",
    "                    train_Gamma_c_mean[traj_idx] + train_Gamma_c_std[traj_idx],\n",
    "                    alpha=0.3, color='red', label='±1 std')\n",
    "    \n",
    "    # Ground truth\n",
    "    ax.plot(time_steps, train_truth['Gamma_c'][traj_idx], 'k-', linewidth=1.5, \n",
    "            label='Ground truth', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Time Step', fontsize=11)\n",
    "    ax.set_ylabel('Gamma_c', fontsize=11)\n",
    "    ax.set_title(f'Training Trajectory {traj_idx + 1}: Gamma_c', fontsize=12)\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path + f\"ensemble_train_trajectories_r{r}_k{num_models_used}.png\", \n",
    "            dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {output_path}ensemble_train_trajectories_r{r}_k{num_models_used}.png\")\n",
    "plt.show()\n",
    "\n",
    "###########################\n",
    "# PLOT 2: Test Trajectories\n",
    "###########################\n",
    "fig, axes = plt.subplots(n_test_traj, 2, figsize=(16, 4*n_test_traj))\n",
    "if n_test_traj == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for traj_idx in range(n_test_traj):\n",
    "    time_steps = np.arange(len(test_Gamma_n_mean[traj_idx]))\n",
    "    \n",
    "    # Gamma_n\n",
    "    ax = axes[traj_idx, 0]\n",
    "    \n",
    "    # Ensemble mean\n",
    "    ax.plot(time_steps, test_Gamma_n_mean[traj_idx], 'b-', linewidth=2, label='Ensemble mean')\n",
    "    \n",
    "    # Uncertainty band\n",
    "    ax.fill_between(time_steps,\n",
    "                    test_Gamma_n_mean[traj_idx] - test_Gamma_n_std[traj_idx],\n",
    "                    test_Gamma_n_mean[traj_idx] + test_Gamma_n_std[traj_idx],\n",
    "                    alpha=0.3, color='blue', label='±1 std')\n",
    "    \n",
    "    # Ground truth\n",
    "    ax.plot(time_steps, test_truth['Gamma_n'][traj_idx], 'k-', linewidth=1.5, \n",
    "            label='Ground truth', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Time Step', fontsize=11)\n",
    "    ax.set_ylabel('Gamma_n', fontsize=11)\n",
    "    ax.set_title(f'Test Trajectory {traj_idx + 1}: Gamma_n', fontsize=12)\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    \n",
    "    # Gamma_c\n",
    "    ax = axes[traj_idx, 1]\n",
    "    \n",
    "    # Ensemble mean\n",
    "    ax.plot(time_steps, test_Gamma_c_mean[traj_idx], 'r-', linewidth=2, label='Ensemble mean')\n",
    "    \n",
    "    # Uncertainty band\n",
    "    ax.fill_between(time_steps,\n",
    "                    test_Gamma_c_mean[traj_idx] - test_Gamma_c_std[traj_idx],\n",
    "                    test_Gamma_c_mean[traj_idx] + test_Gamma_c_std[traj_idx],\n",
    "                    alpha=0.3, color='red', label='±1 std')\n",
    "    \n",
    "    # Ground truth\n",
    "    ax.plot(time_steps, test_truth['Gamma_c'][traj_idx], 'k-', linewidth=1.5, \n",
    "            label='Ground truth', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Time Step', fontsize=11)\n",
    "    ax.set_ylabel('Gamma_c', fontsize=11)\n",
    "    ax.set_title(f'Test Trajectory {traj_idx + 1}: Gamma_c', fontsize=12)\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path + f\"ensemble_test_trajectories_r{r}_k{num_models_used}.png\", \n",
    "            dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {output_path}ensemble_test_trajectories_r{r}_k{num_models_used}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute quantitative error metrics\n",
    "bprint(\"Computing error metrics...\")\n",
    "\n",
    "def compute_errors(pred_mean_list, truth_list, name=\"\"):\n",
    "    \"\"\"Compute MAE and relative errors for a set of trajectories\"\"\"\n",
    "    maes = []\n",
    "    rel_errors = []\n",
    "    \n",
    "    for i, (pred, truth) in enumerate(zip(pred_mean_list, truth_list)):\n",
    "        # Ensure same length\n",
    "        min_len = min(len(pred), len(truth))\n",
    "        pred = pred[:min_len]\n",
    "        truth = truth[:min_len]\n",
    "        \n",
    "        mae = np.mean(np.abs(pred - truth))\n",
    "        rel_error = mae / (np.mean(np.abs(truth)) + 1e-10)\n",
    "        \n",
    "        maes.append(mae)\n",
    "        rel_errors.append(rel_error)\n",
    "        \n",
    "        print(f\"  {name} Traj {i+1}: MAE={mae:.6f}, Rel Error={rel_error:.4%}\")\n",
    "    \n",
    "    print(f\"\\n  {name} Average: MAE={np.mean(maes):.6f}, Rel Error={np.mean(rel_errors):.4%}\")\n",
    "    return maes, rel_errors\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING RECONSTRUCTION ERRORS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGamma_n:\")\n",
    "train_Gamma_n_maes, train_Gamma_n_rels = compute_errors(\n",
    "    train_Gamma_n_mean, train_truth['Gamma_n'], \"Train Gamma_n\"\n",
    ")\n",
    "\n",
    "print(\"\\nGamma_c:\")\n",
    "train_Gamma_c_maes, train_Gamma_c_rels = compute_errors(\n",
    "    train_Gamma_c_mean, train_truth['Gamma_c'], \"Train Gamma_c\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST PREDICTION ERRORS (UNSEEN ICs)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGamma_n:\")\n",
    "test_Gamma_n_maes, test_Gamma_n_rels = compute_errors(\n",
    "    test_Gamma_n_mean, test_truth['Gamma_n'], \"Test Gamma_n\"\n",
    ")\n",
    "\n",
    "print(\"\\nGamma_c:\")\n",
    "test_Gamma_c_maes, test_Gamma_c_rels = compute_errors(\n",
    "    test_Gamma_c_mean, test_truth['Gamma_c'], \"Test Gamma_c\"\n",
    ")\n",
    "\n",
    "# Compute average uncertainty\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE UNCERTAINTY (Avg Std Dev)\")\n",
    "print(\"=\"*60)\n",
    "for i in range(n_train_traj):\n",
    "    print(f\"  Train Traj {i+1}: Gamma_n std={np.mean(train_Gamma_n_std[i]):.6f}, \"\n",
    "          f\"Gamma_c std={np.mean(train_Gamma_c_std[i]):.6f}\")\n",
    "\n",
    "for i in range(n_test_traj):\n",
    "    print(f\"  Test Traj {i+1}: Gamma_n std={np.mean(test_Gamma_n_std[i]):.6f}, \"\n",
    "          f\"Gamma_c std={np.mean(test_Gamma_c_std[i]):.6f}\")\n",
    "\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Save metrics\n",
    "np.savez(\n",
    "    output_path + f\"ensemble_metrics_r{r}_k{num_models_used}.npz\",\n",
    "    train_Gamma_n_maes=train_Gamma_n_maes,\n",
    "    train_Gamma_n_rels=train_Gamma_n_rels,\n",
    "    train_Gamma_c_maes=train_Gamma_c_maes,\n",
    "    train_Gamma_c_rels=train_Gamma_c_rels,\n",
    "    test_Gamma_n_maes=test_Gamma_n_maes,\n",
    "    test_Gamma_n_rels=test_Gamma_n_rels,\n",
    "    test_Gamma_c_maes=test_Gamma_c_maes,\n",
    "    test_Gamma_c_rels=test_Gamma_c_rels\n",
    ")\n",
    "print(f\"Metrics saved to: {output_path}ensemble_metrics_r{r}_k{num_models_used}.npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
