{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6bd3c1-a593-49f9-94e6-e528c238ab9f",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d14478-25a3-4cd2-bd45-7f7ce4c2490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mUsing Device: cuda\n",
      "\u001b[1mShowing animations: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "from IPython.display import HTML, display\n",
    "from neuralop.models import FNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import AdamW\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "from neuralop.data.datasets import load_darcy_flow_small\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "show_animations = True\n",
    "data_truncation = .5\n",
    "\n",
    "plt.rcParams['animation.embed_limit'] = 500\n",
    "\n",
    "print(f\"\\033[1mUsing Device: {device}\")\n",
    "print(f\"\\033[1mShowing animations: {show_animations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d526d6-168c-46fb-803c-caaa8e3d9334",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8723a1eb-72ba-4065-8e3e-019e64768e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and save as .np\n",
    "\n",
    "data_dir = \"/work/10407/anthony50102/frontera/data/hw2d_sim/t600_d256x256_raw/\"\n",
    "\n",
    "train_files = [\"hw2d_sim_step0.025_end1_pts512_c11_k015_N3_nu5e-8_20250315142044_11702_0.h5\",\n",
    "               \"hw2d_sim_step0.025_end1_pts512_c11_k015_N3_nu5e-8_20250315142045_4677_2.h5\"]\n",
    "test_files = [\"hw2d_sim_step0.025_end1_pts512_c11_k015_N3_nu5e-8_20250316215751_19984_3.h5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f5d267-2dd0-43ee-8117-70aa81462367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(density, potential, gamma_n, gamma_c):\n",
    "    data = np.concatenate(\n",
    "        (np.expand_dims(density, 1), np.expand_dims(potential, 1)),\n",
    "        axis=1)\n",
    "    derived_data = np.concatenate(\n",
    "        (np.expand_dims(gamma_n, 1), np.expand_dims(gamma_c, 1)),\n",
    "        axis=1)\n",
    "\n",
    "    return data, derived_data\n",
    "\n",
    "\n",
    "processed_train_files = []\n",
    "\n",
    "for file in train_files:\n",
    "    with h5py.File(data_dir + file, 'r') as f:\n",
    "        end_index = int(f['density'].shape[0] * data_truncation)\n",
    "        density = f['density'][:end_index]\n",
    "        potential = f['phi'][:end_index]\n",
    "        gamma_n = f['gamma_n'][:end_index]\n",
    "        gamma_c = f['gamma_c'][:end_index]\n",
    "        data, derived_data = process(density, potential, gamma_n, gamma_n)\n",
    "\n",
    "        save_name = \"\".join(file.split(\".\")[:-1]) + \".npz\"\n",
    "        processed_train_files.append(save_name)\n",
    "\n",
    "        np.savez(\n",
    "            \"train_\" + save_name,\n",
    "            data=data,\n",
    "            derived_data=derived_data\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eef5b28-d867-4893-99ef-a24b1bf3783b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Animation of the training data\n",
    "if show_animations:\n",
    "    train_data = np.load(processed_train_files[0])[\"data\"][::25]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    vmin = train_data[:, 0, ...].min()\n",
    "    vmax = train_data[:, 0, ...].max()\n",
    "\n",
    "    img = plt.imshow(train_data[0, 0, ...], vmin=vmin, vmax=vmax)\n",
    "\n",
    "    def animate(frame):\n",
    "        img.set_data(train_data[frame, 0, ...])\n",
    "        return [img]\n",
    "\n",
    "    plt.rcParams['animation.embed_limit'] = 500\n",
    "    animation = anim.FuncAnimation(fig, animate, frames=int(train_data.shape[0]), interval=20, blit=True)\n",
    "\n",
    "    HTML(animation.to_jshtml())\n",
    "else:\n",
    "    print(\"Animation is turned off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d7158-02dc-4b26-8540-d093384b41fa",
   "metadata": {},
   "source": [
    "### Dataset and Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47baa21-4944-4c2e-9f01-600e2ef7e94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 2, 256, 256])\n",
      "Target shape: torch.Size([32, 2, 256, 256])\n",
      "torch.Size([1, 801, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class ForwardPredictionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for learning forward prediction operators.\n",
    "\n",
    "    Args:\n",
    "        data_path: Path to the numpy array file\n",
    "        input_steps: Number of past timesteps to use as input\n",
    "        output_steps: Number of future timesteps to predict\n",
    "        stride: Stride between consecutive samples (default: 1)\n",
    "        train_split: Fraction of data to use for training (default: 0.8)\n",
    "        mode: 'train', 'val', or 'test'\n",
    "        val_split: Fraction of remaining data for validation (default: 0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, input_steps=1, output_steps=1, stride=1,\n",
    "                 train_split=0.8, val_split=0.5, mode='train'):\n",
    "        # Load data: (t, channel, x_dim, y_dim)\n",
    "        self.data = np.load(data_path)[\"data\"]\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.stride = stride\n",
    "\n",
    "        total_steps = self.data.shape[0]\n",
    "        sequence_length = input_steps + output_steps\n",
    "\n",
    "        # Split data temporally\n",
    "        train_end = int(total_steps * train_split)\n",
    "        val_end = train_end + int((total_steps - train_end) * val_split)\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.data = self.data[:train_end]\n",
    "        elif mode == 'val':\n",
    "            self.data = self.data[train_end:val_end]\n",
    "        elif mode == 'test':\n",
    "            self.data = self.data[val_end:]\n",
    "        else:\n",
    "            raise ValueError(f\"mode must be 'train', 'val', or 'test', got {mode}\")\n",
    "\n",
    "        # Calculate number of valid sequences\n",
    "        self.num_sequences = (len(self.data) - sequence_length) // stride + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate start index\n",
    "        start_idx = idx * self.stride\n",
    "\n",
    "        # Extract input and output sequences\n",
    "        input_seq = self.data[start_idx:start_idx + self.input_steps]\n",
    "        output_seq = self.data[start_idx + self.input_steps:\n",
    "                                start_idx + self.input_steps + self.output_steps]\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        # TODO: Squeezing out the time dimension fix this\n",
    "        input_tensor = torch.from_numpy(input_seq).float().squeeze(0)  \n",
    "        output_tensor = torch.from_numpy(output_seq).float().squeeze(0)\n",
    "\n",
    "        return {'x': input_tensor,\n",
    "                'y': output_tensor,\n",
    "                't': start_idx,\n",
    "                'tend': start_idx + self.input_steps + self.output_steps - 1}\n",
    "\n",
    "\n",
    "class TrajDataset(Dataset):\n",
    "    # TODO: Look into LRU or memory mapping too speed this up\n",
    "    def __init__(self, data_path,\n",
    "                 train_split=0.8,\n",
    "                 val_split=0.5,\n",
    "                 mode='train'):\n",
    "        # Get all .npz files with full paths\n",
    "        self.data_path = data_path\n",
    "        all_files = [os.path.join(data_path, f) \n",
    "                     for f in os.listdir(data_path) \n",
    "                     if f.endswith(\".npz\")]\n",
    "\n",
    "        self.num_files = len(all_files)\n",
    "\n",
    "        # Split files\n",
    "        train_end = int(self.num_files * train_split)\n",
    "        val_end = train_end + int((self.num_files - train_end) * val_split)\n",
    "\n",
    "        self.train_files = all_files[:train_end]\n",
    "        self.val_files = all_files[train_end:val_end]\n",
    "        self.test_files = all_files[val_end:]\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.avail_files = self.train_files\n",
    "        elif mode == 'val':\n",
    "            self.avail_files = self.val_files\n",
    "        elif mode == 'test':\n",
    "            self.avail_files = self.test_files\n",
    "        else:\n",
    "            raise ValueError(f\"mode must be 'train', 'val', or 'test', got {mode}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.avail_files)  # Fixed: should be length of available files, not total\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.avail_files[idx])[\"data\"]\n",
    "        return {'data': data}\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def create_dataloaders(data_path, input_steps=4, output_steps=1, \n",
    "                       batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test dataloaders.\n",
    "    \"\"\"\n",
    "    train_dataset = ForwardPredictionDataset(\n",
    "        data_path, input_steps=input_steps, output_steps=output_steps, mode='train'\n",
    "    )\n",
    "    val_dataset = ForwardPredictionDataset(\n",
    "        data_path, input_steps=input_steps, output_steps=output_steps, mode='val'\n",
    "    )\n",
    "    test_dataset = ForwardPredictionDataset(\n",
    "        data_path, input_steps=input_steps, output_steps=output_steps, mode='test'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                            shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                             shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def create_traj_loader(data_path, batch_size=1, num_workers=1):\n",
    "    dataset = TrajDataset(data_path)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "data_path = processed_train_files[0]\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    data_path,\n",
    "    input_steps=1,    # Use 4 past timesteps\n",
    "    output_steps=1,   # Predict 1 future timestep\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "traj_loader = create_traj_loader(\".\")\n",
    "\n",
    "# Test the loader\n",
    "for sample in train_loader:\n",
    "    print(f\"Input shape: {sample['x'].shape}\")   # (batch, input_steps, channels, x_dim, y_dim)\n",
    "    print(f\"Target shape: {sample['y'].shape}\")  # (batch, output_steps, channels, x_dim, y_dim)\n",
    "    break\n",
    "\n",
    "for traj in traj_loader:\n",
    "    print(traj['data'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f4108-1266-4894-9db5-7cc3950b1b57",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4750d74-d0aa-411d-a09b-e9dd9e7a3ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 71503746 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = FNO(\n",
    "    n_modes=(32, 32),\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    "    hidden_channels=128,\n",
    "    projection_channel_ratio=2,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count and display the number of parameters\n",
    "n_params = count_model_params(model)\n",
    "print(f\"\\nOur model has {n_params} parameters.\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef2beec-b831-40ca-b8f9-60605ff9a5c7",
   "metadata": {},
   "source": [
    "### Define optim, scheduler, loss funcs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d153fe31-5578-45ef-90e6-960c272b1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "l2loss = LpLoss(d=2, p=2)  # L2 loss for function values\n",
    "h1loss = H1Loss(d=2)  # H1 loss includes gradient information\n",
    "\n",
    "train_loss = h1loss\n",
    "eval_losses = {\"h1\": h1loss, \"l2\": l2loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8d59d-d0aa-4767-b118-433e49a5ae4d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a4ae0-f168-454c-8a05-47309d01eb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 58726.2566, Rollout: 1\n",
      "Epoch 2, Loss: 10067.7461, Rollout: 1\n",
      "Epoch 3, Loss: 474.8905, Rollout: 1\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "rollout = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Phase 1: Single-step warmup (epochs 0-29)\n",
    "    if epoch < 100:\n",
    "        max_rollout = rollout\n",
    "        n_random = 500\n",
    "\n",
    "    # Phase 2: Gradual rollout introduction (epochs 30-99)\n",
    "    elif epoch < 200:\n",
    "        if (epoch - 30) % 10 == 0:  # Increase every 10 epochs instead of 5\n",
    "            rollout += 1\n",
    "        max_rollout = min(rollout, 10)  # Cap at 10 during this phase\n",
    "        n_random = 100  # Keep more single-step training\n",
    "\n",
    "    # Phase 3: Moderate rollouts (epochs 100-199)\n",
    "    elif epoch < 300:\n",
    "        if (epoch - 100) % 8 == 0:  # Increase every 8 epochs\n",
    "            rollout += 1\n",
    "        max_rollout = min(rollout, 25)  # Cap at 25\n",
    "        n_random = 50\n",
    "\n",
    "    # Phase 4: Long rollouts with stability (epochs 200+)\n",
    "    else:\n",
    "        if (epoch - 200) % 10 == 0:  # Slow growth\n",
    "            rollout += 1\n",
    "        max_rollout = min(rollout, 40)  # Hard cap at 40 steps\n",
    "        n_random = 30  # Maintain single-step accuracy\n",
    "\n",
    "    # Training loop\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in traj_loader:\n",
    "        traj = batch['data'].to(device).float()\n",
    "        batch_size, traj_len, c, h, w = traj.shape\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0\n",
    "\n",
    "        # Random single-step predictions\n",
    "        for i in range(n_random):\n",
    "            idx = random.randint(0, traj_len - 2)\n",
    "            pred = model(traj[:, idx])\n",
    "            loss = nn.functional.mse_loss(pred, traj[:, idx + 1])\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        # Autoregressive rollout with gradient clipping\n",
    "        if max_rollout > 1:\n",
    "            start = random.randint(0, traj_len - max_rollout - 1)\n",
    "            state = traj[:, start]\n",
    "\n",
    "            for step in range(max_rollout):\n",
    "                pred = model(state)\n",
    "                target = traj[:, start + step + 1]\n",
    "                loss = nn.functional.mse_loss(pred, target)\n",
    "                loss.backward()\n",
    "                batch_loss += loss.item()\n",
    "                state = pred.detach()\n",
    "\n",
    "                # Gradient clipping for stability\n",
    "                if (step + 1) % 10 == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "        # Clip gradients before final step\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += batch_loss\n",
    "        n_batches += 1\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/n_batches:.4f}, Rollout: {max_rollout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827b4e2-b04c-412e-a948-33d7eb701d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     n_epochs=15,\n",
    "#     device=device,\n",
    "#     wandb_log=False,  # Disable Weights & Biases logging for this tutorial\n",
    "#     eval_interval=5,  # Evaluate every 5 epochs\n",
    "#     use_distributed=False,  # Single GPU/CPU training\n",
    "#     verbose=True,  # Print training progress\n",
    "# )\n",
    "\n",
    "# train_loader, test_loaders, data_processor = load_darcy_flow_small(\n",
    "#     n_train=1000,\n",
    "#     batch_size=64,\n",
    "#     n_tests=[100, 50],\n",
    "#     test_resolutions=[16, 32],\n",
    "#     test_batch_sizes=[32, 32],\n",
    "# )\n",
    "\n",
    "# trainer.train(\n",
    "#     train_loader=train_loader,\n",
    "#     test_loaders={256:test_loader},\n",
    "#     optimizer=optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     regularizer=False,\n",
    "#     training_loss=train_loss,\n",
    "#     eval_losses=eval_losses,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e650d0d-7be0-4943-8fac-97ca1292d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, data, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Autoregressive rollout:\n",
    "    - data: numpy array of shape (T, C, X, Y)\n",
    "    - model: forward prediction operator\n",
    "    - returns: reconstruction of full trajectory\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    T, C, X, Y = data.shape\n",
    "\n",
    "    # Storage for reconstruction\n",
    "    recon = np.zeros_like(data)\n",
    "\n",
    "    # Initial condition (t=0)\n",
    "    current = torch.from_numpy(data[0]).float().to(device)\n",
    "    recon[0] = data[0]\n",
    "\n",
    "    for t in range(1, T):\n",
    "        # Model expects batch dimension\n",
    "        inp = current.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(inp)  # output shape: (1, C, X, Y)\n",
    "        pred_np = pred.squeeze(0).cpu().numpy()\n",
    "        recon[t] = pred_np\n",
    "        # Feed output back in\n",
    "        current = pred.squeeze(0)\n",
    "\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e30a6c-1bd2-467a-9ad5-d69d92438393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset (not split)\n",
    "full_data = np.load(data_path)[\"data\"]\n",
    "print(f\"Full data shape: {full_data.shape}\")\n",
    "\n",
    "# Determine temporal splits you used\n",
    "total_steps = full_data.shape[0]\n",
    "train_end = int(total_steps * 0.8)\n",
    "val_end = train_end + int((total_steps - train_end) * 0.5)\n",
    "\n",
    "train_data = full_data[:train_end]\n",
    "test_data  = full_data[val_end:]\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Run rollouts\n",
    "train_recon = rollout(model, train_data, device=device)\n",
    "test_recon  = rollout(model, test_data, device=device)\n",
    "\n",
    "print(\"Train recon shape:\", train_recon.shape)\n",
    "print(\"Test  recon shape:\", test_recon.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616739e5-7e10-47ea-956c-72a1c442d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_animations:\n",
    "    print(\"\\033[1mShowing animation\")\n",
    "    # Subsample\n",
    "    train_data_subbed = train_data[::5]\n",
    "    train_recon_subbed = train_recon[::5]\n",
    "\n",
    "    # Figure + axes\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    t_vmin = train_data_subbed[:, 0].min()\n",
    "    t_vmax = train_data_subbed[:, 0].max()\n",
    "    # r_vmin = train_recon_subbed[:, 0].min()\n",
    "    # r_vmax = train_recon_subbed[:, 0].max()\n",
    "\n",
    "    # Create the images on the correct axes\n",
    "    img = ax[0].imshow(train_data_subbed[0, 0], vmin=t_vmin, vmax=t_vmax)\n",
    "    img2 = ax[1].imshow(train_recon_subbed[0, 0])\n",
    "\n",
    "    ax[0].set_title(\"Ground Truth\")\n",
    "    ax[1].set_title(\"Reconstruction\")\n",
    "\n",
    "\n",
    "    # Animation function\n",
    "    def animate(frame):\n",
    "        img.set_data(train_data_subbed[frame, 0])\n",
    "        img2.set_data(train_recon_subbed[frame, 0])\n",
    "        return [img, img2]\n",
    "\n",
    "\n",
    "    animation = anim.FuncAnimation(\n",
    "        fig,\n",
    "        animate,\n",
    "        frames=train_data_subbed.shape[0],\n",
    "        interval=20,\n",
    "        blit=True\n",
    "    )\n",
    "\n",
    "    display(HTML(animation.to_jshtml()))\n",
    "else:\n",
    "    print(\"Not showing animations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf3376-b479-42e9-b3d5-109e2e1bbc38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
